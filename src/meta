#!/usr/bin/env python3
# -*- coding: UTF-8 -*-

"""meta: generates metadata for file uploads into object storage
About:
      This program generates the required metedata to push local files into object
    storage in HPC DME for archival purposes. This program ensures that mandatory
    metadata reuirements are fulfilled prior to pushing data into HPC DME.
      Please note that the collection or path in HPC DME that is specified by the
    '--output' option must exist or must be created in HPC DME prior to running
    this program.
USAGE:
	$ meta <sample|combined> [OPTIONS]
Example:
    $ meta sample --input /path/to/data/WType1.{bam,R1.fastq.gz} \
                  --sample-name WType1 \
                  --output /CCBR_Archive/PI_Lab_KenAda_LP/Project_JoeJi_KenAda_Brain_465RNA-seq_2020-12-08/Sample_WT1_WType1
    $ meta combined --input /path/to/data/*.{tsv,html,txt} \
                    --output /CCBR_Archive/PI_Lab_KenAda_LP/Project_JoeJi_KenAda_Brain_465RNA-seq_2020-12-08/Primary_Analysis_RNA-seq_465samples_hg38_35
"""

from __future__ import print_function
import sys, os, json


__author__ = 'Skyler Kuhn'
__version__ = 'v0.1.0'
__email__ = 'kuhnsa@nih.gov'


def exists(testpath):
    """Checks if file exists on the local filesystem.
    @param testpath <str>:
        Name of file/directory to check
    @return does_exist <boolean>:
        True when file/directory exists, False when file/directory does not exist
    """
    does_exist = True
    if not os.path.exists(testpath):
        does_exist = False # File or directory does not exist on the filesystem

    return does_exist


def permissions(parser, filename, *args, **kwargs):
    """Checks permissions using os.access() to see the user is authorized to access
    a file/directory. Checks for existence, readability, writability and executability via:
    os.F_OK (tests existence), os.R_OK (tests read), os.W_OK (tests write), os.X_OK (tests exec).
    @param parser <argparse.ArgumentParser() object>:
        Argparse parser object
    @param filename <str>:
        Name of file to check
    @return filename <str>:
        If file exists and user can read from file
    """
    if not exists(filename):
        parser.error("File '{}' does not exists! Failed to provided vaild input.".format(filename))

    if not os.access(filename, *args, **kwargs):
        parser.error("File '{}' exists, but cannot read file due to permissions!".format(filename))

    return filename


def _cp_r_safe_(source, target, resources = []):
    """Private function: Given a list paths it will recursively copy each to the
    target location. If a target path already exists, it will NOT over-write the
    existing paths data.
    @param resources <list[str]>:
        List of paths to copy over to target location
    @params source <str>:
        Add a prefix PATH to each resource
    @param target <str>:
        Target path to copy templates and required resources
    """
    from shutil import copytree

    for resource in resources:
        destination = os.path.join(target, resource)
        if not exists(destination):
            # Required resources do not exist
            copytree(os.path.join(source, resource), destination)

    return


def md5sum(filename, blocksize = 65536):
    """Gets md5checksum of a file in memory-safe manner.
    The file is read in blocks defined by the blocksize parameter. This is a safer
    option to reading the entire file into memory if the file is very large.
    @param filename <str>:
        Input file on local filesystem to find md5 checksum
    @param blocksize <int>:
        Blocksize of reading N chunks of data to reduce memory profile
    @return hasher.hexdigest() <str>:
        MD5 checksum of the file's contents
    """
    import hashlib

    hasher = hashlib.md5()
    with open(filename, 'rb') as fh:
        buf = fh.read(blocksize)
        while len(buf) > 0:
            hasher.update(buf)
            buf = fh.read(blocksize)

    return hasher.hexdigest()


def compressed(file_extension):
    """Determines if a file is compressed based on its file extension.
    @param file_extension <str>:
        File extension of input file
    @return data_compression <str>:
        DME controlled vocabulary for file compression status
    """
    data_compression = "Not Compressed"
    if file_extension in ["bz2", "gz", "bam", "xz", "rar", "tar", "tbz2", "tgz", "zip", "7z"]:
        data_compression = "Compressed"

    return data_compression


def generate_json(metadata, output_filename):
    """Generate metadata json file expected by dm_register_directory command
    @param metadata <dictionary>:
        Dictionary containing key,value pairs of attributes and values
    @param output_filename <str>:
         Metadata json file expected by dm_register_directory command
    """
    # Save upload data-object metadata data as JSON file
    print('Creating {}'.format(output_filename))
    with open(output_filename, 'w') as file:
        json.dump(metadata, file, sort_keys=True, indent=4)

    return


def ftype(filename):
    """Infers the file type from the filename (FASTQ, BAM, COUNTS, TSV, HTML)
    @param filename <str>:
        Name of file to infer the file type
    @return filetype <str>:
        Inferred file type (FASTQ, BAM, COUNTS, TSV, HTML)
    """
    filename = filename.rstrip('.gz')
    filetype = filename.split('.')[-1].upper()

    # Check for common edge-cases
    if 'counts' in filename and filetype not in ['MD5', 'JSON']:
        filetype = 'COUNTS'
    elif 'fastq' in filename and filetype not in ['MD5', 'JSON']:
        filetype = 'FASTQ'

    return filetype



def minimal_common_metadata(input_file, dme_path):
    """Get common required metadata across sample and combined data.
    @param input_file <str>:
        Input file on local filesystem to archive
    @param dme_path <str>:
        Path or collection in HPC DME to archive the file
    @return metadata <dictionary>:
        Dictionary containing metadata values and attributes of the file to upload
    """
    # Get minimal required metadata
    sample = os.path.basename(input_file)

    # Metadata Template
    metadata = \
    {
        "metadataEntries": [
            {
                "attribute": "phi_content",
                "value": "Unspecified"
            },
            {
                "attribute": "pii_content",
                "value": "Unspecified"
            },
            {
                "attribute": "data_encryption_status",
                "value": "Unspecified"
            },
            {
                "attribute": "analysis_team",
                "value": "CCBR"
            },
            {
                "attribute": "object_name",
                "value": os.path.join(dme_path, sample)
            },
            {
                "attribute": "alias",
                "value": os.path.realpath(os.path.abspath(input_file))
            },
            {
                "attribute": "file_type",
                "value": ftype(sample)
            },
            {
                "attribute": "data_compression_status",
                "value": compressed(sample.split('.')[-1].lower())
            },
            {
                "attribute": "md5_checksum",
                "value": md5sum(input_file)
            },


        ]
    }

    return metadata


def sample(sub_args):
    """Generates required metadata single sample data/files (bams, fastqs)
    into HPC DME.
    @param sub_args <parser.parse_args() object>:
        Parsed arguments for sample sub-command
    """

    for file in sub_args.input:
        metadata = minimal_common_metadata(input_file = file, dme_path = sub_args.output)
        if sub_args.sample_name:
            metadata["metadataEntries"].append({"attribute": "sample_name", "value": str(sub_args.sample_name)})
        if sub_args.analysis_id:
            metadata["metadataEntries"].append({"attribute": "md5_all_inputs", "value": str(sub_args.analysis_id)})
            try:
                serial_md5 = "{}-{}-{}".format(str(sub_args.analysis_id[:3]),
                    str(sub_args.analysis_id[round(len(sub_args.analysis_id)/2):(round(len(sub_args.analysis_id)/2))+2]),
                    str(sub_args.analysis_id[-4:]))
                metadata["metadataEntries"].append({"attribute": "md5_all_inputs_serial", "value": str(serial_md5)})
            except IndexError:
                pass
        if sub_args.dme_analysis_collection:
            metadata["metadataEntries"].append({"attribute": "analysis_collection", "value": str(sub_args.dme_analysis_collection)})
        output_file = os.path.abspath(file) + ".metadata.json"
        generate_json(metadata = metadata, output_filename = output_file)

    return


def combined(sub_args):
    """Generates required metadata for multi-sample data/files (Report, Counts Matrix)
    into HPC DME. The metadata requirements for pushing multi-sample files into HPC DME
    differ from single sample data.
    @param sub_args <parser.parse_args() object>:
        Parsed arguments for sample sub-command
    """
    for file in sub_args.input:
        metadata = minimal_common_metadata(input_file = file, dme_path = sub_args.output)
        if sub_args.analysis_id:
            metadata["metadataEntries"].append({"attribute": "md5_all_inputs", "value": str(sub_args.analysis_id)})
            try:
                serial_md5 = "{}-{}-{}".format(str(sub_args.analysis_id[:3]),
                    str(sub_args.analysis_id[round(len(sub_args.analysis_id)/2):(round(len(sub_args.analysis_id)/2))+2]),
                    str(sub_args.analysis_id[-4:]))
                metadata["metadataEntries"].append({"attribute": "md5_all_inputs_serial", "value": str(serial_md5)})
            except IndexError:
                pass
        output_file = os.path.abspath(file) + ".metadata.json"
        generate_json(metadata = metadata, output_filename = output_file)

    return


def parsed_arguments():
    """Parses user-provided command-line arguments. Requires argparse package.
    """
    import argparse

    # Create a top-level parser
    parser = argparse.ArgumentParser(description = 'meta: \
                                                    a utility to generate metadata for local files \
                                                    prior to uploading into object storage.')

    # Adding Verison information
    parser.add_argument('--version', action = 'version', version='%(prog)s {}'.format(__version__))

    # Create sub-command parser
    subparsers = parser.add_subparsers()

    # Options for the "sample" sub-command
    subparser_sample = subparsers.add_parser('sample',
                                            help = 'Generates required single sample metadata (FastQ and BAM) \
                                            for uploading into object storage.',
                                            description = 'Metadata requirements for pushing \
                                            single sample data into object storage differ from multi-sample data.')
    # Input FastQ files
    subparser_sample.add_argument('-i', '--input',
                                # Check if the file exists and if it is readable
                                type = lambda file: permissions(parser, file, os.R_OK),
                                required = True,
                                nargs = '+',
                                help = 'Required: Input single sample file to be uploaded (i.e. FastQ or BAM). \
                                        One file must be provided. \
                                        Example: --input *.fastq.gz')
    # Output Collection in HPC DME
    subparser_sample.add_argument('-o', '--output',
                                type = str,
                                required = True,
                                help = 'Required: Path/Collection in HPC DME to archive the file. \
                                        This is the absolute PATH in HPC DME where the file will be archived. \
                                        Please note each collection MUST be intialized prior to running meta. \
                                        Example: --output /Archive/PI_Lab/Project/Sample')

    # Strip extension from filename
    subparser_sample.add_argument('-s', '--sample-name',
                                type = str,
                                required = False,
                                help = 'Optional: Sample name the file is associated with. \
                                        This is the name of the sample that the file is associated with, \
                                        any file extensions should be remove from this string. \
                                        Example: --sample-name WType1')

    # Primary Analysis ID associated with a Sample
    subparser_sample.add_argument('-a', '--analysis-id',
                                type = str,
                                required = False,
                                help = 'Optional: Primary Analysis ID of the Pipeline which generated the input files. \
                                        The Primary Analysis ID is a unique identifer for a given pipeline run. This unique \
                                        identifer is calculated by find the MD5 of all the pipeline inputs.\
                                        Example: --analysis-id 26071405f2f1c3a6f71d4141edb208e2')

    # Primary Analysis Collection associated with a Sample
    subparser_sample.add_argument('-d', '--dme-analysis-collection',
                                type = str,
                                required = False,
                                help = 'Optional: DME Primary Analysis Collection Path associated with a sample. \
                                        A Primary Analysis collection contains multi-sample aggregate data such as \
                                        a counts matrix or reports. This field represents the absolute path in HPC \
                                        DME to a given samples primary analysis results.\
                                        Example: --dme-analysis-collection /CCBR_EXT_Archive/PI_Lab/Project/Primary_Analysis')

    # Options for the "combined" sub-command
    subparser_combined = subparsers.add_parser('combined',
                                            help = 'Generates required multi-sample metadata  \
                                            (Counts Matrix, Reports, Summary file) for uploading \
                                            into object storage.',
                                            description = 'Metadata requirements for pushing \
                                            multi-sample data into object storage differ from per sample data.')
    # Input Multi-sample data files
    subparser_combined.add_argument('-i', '--input',
                                # Check if the file exists and if it is readable
                                type = lambda file: permissions(parser, file, os.R_OK),
                                required = True,
                                nargs = '+',
                                help = 'Required: Input combined file to be uploaded. \
                                        One file must be provided. \
                                        Example: --input .tests/*.html')
    # Output Collection in HPC DME
    subparser_combined.add_argument('-o', '--output',
                                type = str,
                                required = True,
                                help = 'Required: Path/Collection in HPC DME to archive the file. \
                                        This is the absolute PATH in HPC DME where the file will be archived. \
                                        Please note each collection MUST be intialized prior to running meta. \
                                        Example: --output /CCBR_EXT_Archive/PI_Lab/Project/Primary_Analysis')

    # Primary Analysis ID associated with a Multi-sample file
    subparser_combined.add_argument('-a', '--analysis-id',
                                type = str,
                                required = False,
                                help = 'Optional: Primary Analysis ID of the Pipeline which generated the input files. \
                                        The Primary Analysis ID is a unique identifer for a given pipeline run. This unique \
                                        identifer is calculated by find the MD5 of all the pipeline inputs.\
                                        Example: --analysis-id 26071405f2f1c3a6f71d4141edb208e2')

    # Define run() as handler for sub-parser
    subparser_sample.set_defaults(func = sample)
    subparser_combined.set_defaults(func = combined)

    # Parse command-line args
    args = parser.parse_args()
    return args


def main():

    # Collect args for sub-command
    args = parsed_arguments()

    # Mediator method to call sub-command's set handler function
    args.func(args)


if __name__ == '__main__':
    main()
